import streamlit as st
import nest_asyncio
import os
from dotenv import load_dotenv

# Apply the patch for asyncio right at the start
nest_asyncio.apply()
# Load environment variables from .env file for local development
load_dotenv()


import helpers
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import asyncio
import shutil
from langchain.vectorstores import FAISS
import io
import zipfile

def get_zip_file_bytes(vector_store, filename):
    TEMP_DIR = "./temp_download"
    save_path = os.path.join(TEMP_DIR, filename.replace(" ", "_") + "_faiss_index")
    
    if os.path.exists(save_path):
        shutil.rmtree(save_path)
    
    vector_store.save_local(save_path)
    
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED, False) as zip_file:
        for root, _, files in os.walk(save_path):
            for file in files:
                zip_file.write(os.path.join(root, file), 
                               os.path.relpath(os.path.join(root, file), save_path))
    
    shutil.rmtree(TEMP_DIR) # Clean up after zipping
    return zip_buffer.getvalue()

st.set_page_config(page_title="RAG Q&A with Gemini", layout="wide")
st.title("ğŸ“„ RAG-based Q&A with Gemini")

# --- Session State Initialization ---
if "conversation" not in st.session_state:
    st.session_state.conversation = []
if "vector_stores" not in st.session_state:
    st.session_state.vector_stores = {}
if "selected_sources_dict" not in st.session_state:
    st.session_state.selected_sources_dict = {}

# --- Load Default RAG Indexes on Startup ---
DEFAULT_RAG_ROOT = "./RAG_file"
if "default_loaded" not in st.session_state:
    if os.path.exists(DEFAULT_RAG_ROOT) and os.path.isdir(DEFAULT_RAG_ROOT):
        try:
            with st.spinner("Loading default RAG sources..."):
                embeddings = helpers.get_hf_embeddings()
                for item in sorted(os.listdir(DEFAULT_RAG_ROOT), reverse=True):
                    item_path = os.path.join(DEFAULT_RAG_ROOT, item)
                    # Check if it's a directory and not already loaded
                    if os.path.isdir(item_path) and f"{item} (Default)" not in st.session_state.vector_stores:
                        default_vs = FAISS.load_local(item_path, embeddings, allow_dangerous_deserialization=True)
                        st.session_state.vector_stores[f"{item} (Default)"] = default_vs
            if st.session_state.vector_stores:
                st.toast("Default RAG sources loaded successfully!")
        except Exception as e:
            st.error(f"Failed to load default RAG sources: {e}")
    st.session_state.default_loaded = True

# --- Sidebar for Controls ---
with st.sidebar:
    st.header("API é‡‘é‘°è¨­å®š")
    
    user_api_key = st.text_input(
        label="Google API é‡‘é‘° (é¸å¡«)",
        placeholder="è²¼ä¸Šæ‚¨çš„é‡‘é‘°ä»¥è¦†è“‹é è¨­å€¼",
        help="å¦‚æœç•™ç©ºï¼Œå°‡è‡ªå‹•ä½¿ç”¨æ‡‰ç”¨ç¨‹å¼å…§å»ºçš„é è¨­é‡‘é‘°ã€‚",
        type="password"
    )

    google_api_key = user_api_key or os.getenv("GOOGLE_API_KEY")
    default_key_found = os.getenv("GOOGLE_API_KEY") is not None

    # Display a single, consolidated status line
    if user_api_key:
        st.caption("ğŸŸ¢ æ‚¨æ­£åœ¨ä½¿ç”¨è‡ªå·±è¼¸å…¥çš„é‡‘é‘°ã€‚")
    elif default_key_found:
        st.caption("ğŸ”µ å·²è‡ªå‹•è¼‰å…¥é è¨­é‡‘é‘°ã€‚")
    else:
        st.caption("ğŸ”´ æœªæ‰¾åˆ°ä»»ä½•é‡‘é‘°ï¼Œè«‹æ‰‹å‹•è¼¸å…¥ã€‚")
    
    st.markdown("---")
    
    st.header("RAG æ–‡ä»¶ç®¡ç†")
    
    uploaded_files = st.file_uploader("ä¸Šå‚³æ–°çš„ PDF æ–‡ä»¶", type="pdf", accept_multiple_files=True)

    if uploaded_files:
        with st.spinner("Processing documents..."):
            for uploaded_file in uploaded_files:
                if uploaded_file.name not in st.session_state.vector_stores:
                    raw_text = helpers.process_text_from_pdfs([uploaded_file])
                    text_chunks = helpers.get_text_chunks(raw_text)
                    vector_store = helpers.create_vector_store(text_chunks)
                    if vector_store:
                        st.session_state.vector_stores[uploaded_file.name] = vector_store
                        st.success(f"Processed and indexed: {uploaded_file.name}")
                    else:
                        st.warning(f"Could not process text from: {uploaded_file.name}")

    st.subheader("Available RAG Sources")
    if not st.session_state.vector_stores:
        st.info("Upload a document or add a default RAG folder to begin.")
    else:
        # Use a dictionary to track checkbox states
        selected_sources_dict = {}
        for filename, vector_store in st.session_state.vector_stores.items():
            # Display checkbox and store its state
            is_selected = st.checkbox(filename, value=st.session_state.selected_sources_dict.get(filename, True), key=f"cb_{filename}")
            selected_sources_dict[filename] = is_selected
            
            # --- Single-Step Download Logic ---
            st.download_button(
                label="â¬‡ï¸ Download",
                data=get_zip_file_bytes(vector_store, filename),
                file_name=f"{filename}_faiss_index.zip",
                mime="application/zip",
                key=f"dl_{filename}"
            )
            st.markdown("---") # Separator
        
        # Update the master dictionary of checkbox states
        st.session_state.selected_sources_dict = selected_sources_dict

# --- Main Chat Interface ---
for message in st.session_state.conversation:
    with st.chat_message(message["role"]):
        # Check for assistant messages with metadata to display the source
        if message["role"] == "assistant":
            if "is_error" in message:
                 st.error(message["content"])
            else:
                if message.get("rag_used") == False:
                    st.caption("Source: General Knowledge")
                elif message.get("rag_used") == True and "sources" in message:
                    # Format source names for readability
                    formatted_sources = [s.replace("_faiss_index (Default)", "").replace(".pdf", "") for s in message.get("sources", [])]
                    st.caption(f"Source(s): {', '.join(formatted_sources)}")
                st.markdown(message["content"])
        else: # User messages
             st.markdown(message["content"])


if user_question := st.chat_input("Ask a question..."):
    st.session_state.conversation.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    # CRITICAL: Check for API key availability *before* making any calls
    if not google_api_key:
        error_message = "API é‡‘é‘°æœªè¨­å®šã€‚è«‹åœ¨å·¦å´å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google API é‡‘é‘°ï¼Œæˆ–ç”±æ‡‰ç”¨ç¨‹å¼ç®¡ç†å“¡è¨­å®šé è¨­é‡‘é‘°ã€‚"
        st.session_state.conversation.append({"role": "assistant", "content": error_message, "is_error": True})
        with st.chat_message("assistant"):
            st.error(error_message)
    else:
        # Determine which vector stores are selected for the query
        selected_stores_for_query = {
            name: store for name, store in st.session_state.vector_stores.items()
            if st.session_state.selected_sources_dict.get(name, True)
        }

        # --- Query Logic ---
        if not selected_stores_for_query:
            # Fallback to direct LLM if no RAG source is selected
            st.info("æœªé¸å– RAG æ–‡ä»¶ï¼Œä½¿ç”¨ LLM å…§éƒ¨çŸ¥è­˜å›ç­”ã€‚")
            with st.spinner("Thinking..."):
                try:
                    # Use a stable chain pattern similar to the RAG path
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-pro", # Standardizing model
                        temperature=0.7,
                        google_api_key=google_api_key,
                        convert_system_message_to_human=True,
                        request_timeout=120
                    )
                    
                    # Create a simple prompt and chain for direct queries
                    prompt_template = "Question: {question}\n\nAnswer in Traditional Chinese:"
                    prompt = PromptTemplate(template=prompt_template, input_variables=["question"])
                    llm_chain = LLMChain(prompt=prompt, llm=llm)
                    
                    response = llm_chain.run(question=user_question)

                except Exception as e:
                    response = f"å‘¼å« LLM æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š {type(e).__name__} - {e}"
            
            with st.chat_message("assistant"):
                st.caption("Source: General Knowledge")
                st.markdown(response)
            st.session_state.conversation.append({"role": "assistant", "content": response, "rag_used": False})

        else:
            # Use RAG to generate an answer
            with st.spinner("Thinking with RAG..."):
                try:
                    # Call the helper function with the original question
                    response = helpers.generate_answer(
                        user_question,
                        selected_stores_for_query,
                        google_api_key
                    )
                except Exception as e:
                    response = f"RAG è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š {type(e).__name__} - {e}"
            
            with st.chat_message("assistant"):
                source_names = list(selected_stores_for_query.keys())
                formatted_sources = [s.replace("_faiss_index (Default)", "").replace(".pdf", "") for s in source_names]
                st.caption(f"Source(s): {', '.join(formatted_sources)}")
                st.markdown(response)
            st.session_state.conversation.append({"role": "assistant", "content": response, "rag_used": True, "sources": source_names})